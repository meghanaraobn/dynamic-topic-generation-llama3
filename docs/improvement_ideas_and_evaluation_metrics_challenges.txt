=================================================================================================================
Improvement Ideas
=================================================================================================================

- Dataset:

1. Fine-tuning the Llama 3 model with a diverse range of data, including both long and short paragraphs, significantly enhances its capability to generate more relevant topics. 

- Model Architecture

2. Although quantized models significantly reduce VRAM usage, allowing for larger batch sizes and longer context lengths making it feasible to run on lower-end GPUs the quantization can introduce some loss in accuracy and performance due to the lower precision representation of weights and activations. Using the full Llama 3 model which operates with higher precision (16-bit or 32-bit floating point) leads to better performance in terms of accuracy and the ability to capture complex patterns in the data.

- Hyper-parameter Tuning

3. Experimenting with different learning rates, epochs numbers, batch sizes and gradient accumulation steps to check if there is any impact on the model performance.
4. Experimenting with different adaptive learning rate techniques such as Cosine Annealing and Warmup Schedules and different optimization algorithms like Lookahead Optimizers, to improve model performance.

- Evaluation and validation

5. Fine-tuning multiple variants of the Llama 3 model with varying hyperparameters like learning rates, batch sizes, number of epochs and model architectures helps determine the optimal configuration leading to good results.
6. Using an early stopping technique based on validation loss to prevent overfitting and to ensure optimal model performance. Continuously assessing the model's performance on a validation dataset during training to make necessary optimizations.

=================================================================================================================
Ideas on evaluating the generated topic from Llama 3  

Reference - (https://next.redhat.com/2024/05/16/evaluating-the-performance-of-large-language models/#:~:text=Automatic%20evaluation%20metrics%20play%20a,precision%20in%20generating%20identical%20responses.)
=================================================================================================================

1. Automatic Metrics: These are quantitative measures used to assess the quality of generated responses without human intervention. Metrics such as exact match, BLEU scores, ROUGE scores, and word error rate (WER) can be used to evaluate how accurately Llama 3 generates topics for a given text data.

2. Human Evaluation: Human evaluators check if the generated topics are appropriate and sound natural.

3. Hybrid Evaluation Approach: Combining automatic metrics with human evaluation gives a better understanding of how well Llama 3 performs.

4. Feedback Integration: Feedback from both automatic metrics and human evaluators can be integrated into the model's fine-tuning processes. For example, if automatic metrics indicate a low performance on certain types of inputs, then adjustments in training strategies or hyperparameters can be made to improve the model's ability to generate relevant topics.

=================================================================================================================
Challenges
=================================================================================================================

1. Computational Resources: Training Llama3 required substantial GPU and RAM resources beyond what was available on my local system. I utilized Google Colab Pro. Even then I could not use the original Llama 3 model, I had to use a quantized version of the model to accomodate the resources provided by colab pro.

2. Data availability: I invested time searching for a suitable dataset focused specifically on topics essential for meaningful topic generation. Datasets containing titles, topic classes, and summaries were not suitable since the model needed to only learn to generate topics and not anything else.

3. Finding Optimal Hyperparameters: Llama 3, being an LLM, inherently possesses broad topic knowledge. While fine-tuning with fewer steps can yield meaningful results initially, iterative training over different batch sizes, epochs or steps was important to get consistent performance for different text inputs.
